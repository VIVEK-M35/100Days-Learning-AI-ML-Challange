{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl0_ezfJN_vi"
      },
      "source": [
        "# Day 4: Model Selection & Training\n",
        "\n",
        "In this notebook, we'll train multiple machine learning models, evaluate them, and explore cross-validation + hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNYQjT1vN_vj"
      },
      "source": [
        "## Step 1: Load Dataset\n",
        "We are using the Breast Cancer dataset from Scikit-learn. This is a binary classification dataset where the task is to predict whether a tumor is malignant or benign based on medical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0JHh0igN_vj",
        "outputId": "64e75436-a4db-4a8a-99a7-b4ca457251d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((569, 30), (569,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzOpUmM_N_vk"
      },
      "source": [
        "## Step 2: Train/Test Split\n",
        "We split the dataset into training and testing sets so we can train models on one part of the data and evaluate their performance on unseen data. This helps avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt1lP_MhN_vk",
        "outputId": "0d831547-70a0-41de-f53c-a1d395661ea0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((455, 30), (114, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_MeaICdN_vk"
      },
      "source": [
        "## Step 3: Train Baseline Models\n",
        "Here, we train four models:\n",
        "- **Logistic Regression**: A linear model good for baseline classification.\n",
        "- **K-Nearest Neighbors (KNN)**: Classifies based on the majority label of neighbors.\n",
        "- **Decision Tree**: A simple tree-based model that splits data by feature thresholds.\n",
        "- **Random Forest**: An ensemble of decision trees for better generalization.\n",
        "\n",
        "We use a Pipeline with StandardScaler to normalize features before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoilLgbfN_vk",
        "outputId": "8937c44b-8f33-4ab1-acaa-4fc026878b78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Logistic Regression': {'0': {'precision': 0.9761904761904762,\n",
              "   'recall': 0.9761904761904762,\n",
              "   'f1-score': 0.9761904761904762,\n",
              "   'support': 42.0},\n",
              "  '1': {'precision': 0.9861111111111112,\n",
              "   'recall': 0.9861111111111112,\n",
              "   'f1-score': 0.9861111111111112,\n",
              "   'support': 72.0},\n",
              "  'accuracy': 0.9824561403508771,\n",
              "  'macro avg': {'precision': 0.9811507936507937,\n",
              "   'recall': 0.9811507936507937,\n",
              "   'f1-score': 0.9811507936507937,\n",
              "   'support': 114.0},\n",
              "  'weighted avg': {'precision': 0.9824561403508771,\n",
              "   'recall': 0.9824561403508771,\n",
              "   'f1-score': 0.9824561403508771,\n",
              "   'support': 114.0}},\n",
              " 'KNN': {'0': {'precision': 0.9512195121951219,\n",
              "   'recall': 0.9285714285714286,\n",
              "   'f1-score': 0.9397590361445783,\n",
              "   'support': 42.0},\n",
              "  '1': {'precision': 0.958904109589041,\n",
              "   'recall': 0.9722222222222222,\n",
              "   'f1-score': 0.9655172413793104,\n",
              "   'support': 72.0},\n",
              "  'accuracy': 0.956140350877193,\n",
              "  'macro avg': {'precision': 0.9550618108920814,\n",
              "   'recall': 0.9503968253968254,\n",
              "   'f1-score': 0.9526381387619444,\n",
              "   'support': 114.0},\n",
              "  'weighted avg': {'precision': 0.9560729421281235,\n",
              "   'recall': 0.956140350877193,\n",
              "   'f1-score': 0.9560273762928302,\n",
              "   'support': 114.0}},\n",
              " 'Decision Tree': {'0': {'precision': 0.8478260869565217,\n",
              "   'recall': 0.9285714285714286,\n",
              "   'f1-score': 0.8863636363636364,\n",
              "   'support': 42.0},\n",
              "  '1': {'precision': 0.9558823529411765,\n",
              "   'recall': 0.9027777777777778,\n",
              "   'f1-score': 0.9285714285714286,\n",
              "   'support': 72.0},\n",
              "  'accuracy': 0.9122807017543859,\n",
              "  'macro avg': {'precision': 0.9018542199488491,\n",
              "   'recall': 0.9156746031746033,\n",
              "   'f1-score': 0.9074675324675325,\n",
              "   'support': 114.0},\n",
              "  'weighted avg': {'precision': 0.9160721496836722,\n",
              "   'recall': 0.9122807017543859,\n",
              "   'f1-score': 0.9130211893369787,\n",
              "   'support': 114.0}},\n",
              " 'Random Forest': {'0': {'precision': 0.9512195121951219,\n",
              "   'recall': 0.9285714285714286,\n",
              "   'f1-score': 0.9397590361445783,\n",
              "   'support': 42.0},\n",
              "  '1': {'precision': 0.958904109589041,\n",
              "   'recall': 0.9722222222222222,\n",
              "   'f1-score': 0.9655172413793104,\n",
              "   'support': 72.0},\n",
              "  'accuracy': 0.956140350877193,\n",
              "  'macro avg': {'precision': 0.9550618108920814,\n",
              "   'recall': 0.9503968253968254,\n",
              "   'f1-score': 0.9526381387619444,\n",
              "   'support': 114.0},\n",
              "  'weighted avg': {'precision': 0.9560729421281235,\n",
              "   'recall': 0.956140350877193,\n",
              "   'f1-score': 0.9560273762928302,\n",
              "   'support': 114.0}}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    results[name] = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0y_GFLdN_vk"
      },
      "source": [
        "## Step 4: Cross-Validation\n",
        "Cross-validation helps us evaluate models more reliably by splitting data into multiple folds. We train and test on different folds and then average the results to reduce variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfBTUsX0N_vk",
        "outputId": "d21ead64-551a-4f8d-8011-c03dd049a8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy scores: [0.98245614 0.98245614 0.97368421 0.97368421 0.99115044]\n",
            "Mean CV accuracy: 0.9806862288464524\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Cross-validation example with Logistic Regression\n",
        "log_reg = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1000))])\n",
        "cv_scores = cross_val_score(log_reg, X, y, cv=5)\n",
        "print('Cross-validation accuracy scores:', cv_scores)\n",
        "print('Mean CV accuracy:', np.mean(cv_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJxgquv6N_vl"
      },
      "source": [
        "## Step 5: Hyperparameter Tuning with GridSearchCV\n",
        "Models like Random Forest have hyperparameters (e.g., number of trees, depth). GridSearchCV tries different combinations to find the best settings using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKHIndc2N_vl",
        "outputId": "1ddd535e-0c73-4651-9b72-edbcaf41bfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'model__max_depth': 5, 'model__n_estimators': 100}\n",
            "Best CV score: 0.953831183920065\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV (Random Forest)\n",
        "param_grid = {\n",
        "    'model__n_estimators': [50, 100],\n",
        "    'model__max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('model', RandomForestClassifier(random_state=42))])\n",
        "grid = GridSearchCV(pipe, param_grid, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print('Best parameters:', grid.best_params_)\n",
        "print('Best CV score:', grid.best_score_)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}